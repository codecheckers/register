{
  "certificate": {
    "id": "2025-013",
    "url": "https://codecheck.org.uk/register/certs/2025-013/"
  },
  "paper": {
    "title": "Exploring Urban Place Function through User-Generated Textual Content",
    "authors": [
      {
        "name": "Shahreen Muntaha Nawfee",
        "orcid": "0000-0001-6154-8537"
      },
      {
        "name": "Stefano De Sabbata",
        "orcid": "0000-0002-2750-7579"
      },
      {
        "name": "Nicholas Tate",
        "orcid": "0009-0004-7648-5785"
      }
    ],
    "reference": "https://doi.org/10.5194/agile-giss-6-6-2025",
    "abstract": {
      "text": "<jats:p>Abstract. Understanding the functions played by different places as part of the urban fabric is essential for the sustainable development of cities. The increase in the amount and variety of people's digital footprint, combined with the recent emergence of Large Language Models (LLMs), holds the potential to derive novel and useful insights into the function of urban places from unstructured textual content. This paper presents a reproducible and robust methodological framework to extract place functional characteristics from user-generated textual content. In particular, we analyse tags of Points of Interest (POIs) from OpenStreetMap (OSM) and summaries of geo-tagged articles from Wikipedia through spatial clustering and topic modelling, exploring classic and current approaches in Natural Language Processing (NLP). We evaluate and compare the performance of different LLMs, including Mistral and Llama. Our results show that the framework provides a good insight into place functionality for our case study. \n                    <\/jats:p>",
      "source": "CrossRef"
    }
  },
  "codecheck": {
    "codecheckers": [
      {
        "name": "Ilya Ilyankou",
        "orcid": "0009-0008-7082-7122"
      }
    ],
    "check_time": "2025-06-12 12:00:00",
    "repository": "osf::BGY83",
    "report": "https://doi.org/10.17605/OSF.IO/BGY83",
    "type": "conference",
    "venue": "AGILEGIS",
    "summary": "The manuscript was partially reproducible, with three of seven figures recreated with some deviations, likely due to unset random states in the computational process. The remaining figures and tables could not be reproduced in this process due to code issues; the GitHub repository contained 16 code files but lacked sufficient documentation for navigating the complex workflow. While the code can be run on standard hardware with common Python and R libraries, there are no instructions on setup or library versioning. Communication with the authors (17 emails over ~10 days) was necessary to obtain intermediate data files and clarify the process.\n",
    "manifest": [
      {
        "file": "NA",
        "comment": "The AGILE 2025 Reproducibility Review did not include manifest documentation, see https://github.com/codecheckers/register/issues/147"
      }
    ]
  }
}
