{
  "certificate": {
    "id": "2022-002",
    "url": "https://codecheck.org.uk/register/certs/2022-002/"
  },
  "paper": {
    "title": "Understanding the Imperfection of 3D point Cloud and Semantic Segmentation algorithms for 3D Models of Indoor Environment",
    "authors": [
      {
        "name": "Guoray Cai orcid 1 and",
        "orcid": "0000-0002-5189-8442"
      },
      {
        "name": "Yimu Pan"
      }
    ],
    "reference": "https://doi.org/10.5194/agile-giss-3-2-2022",
    "abstract": {
      "text": "<jats:p>Abstract. Point clouds data provides new potentials for automated construction of more geometrically accurate and semantically rich 3D models for indoor environments. Recent advances in deep learning methods on point cloud semantic segmentation demonstrated impressive accuracy in labeling points of 3D surfaces with object classes. However, it remains challenging to reconstruct the shape of semantic objects from semantically-labeled 3D points, due to imperfection of such data and the under-determination of object construction algorithms. We have little empirical knowledge about how data imperfections affect the reconstruction of 3D indoor room objects. This paper contributes to understanding the nature of such imperfection of 3D point cloud data and semantic segmentation algorithms by analyzing the reconstructability of indoor room objects from semantically-labeled point cloud. 181 rooms from Stanford Large-Scale 3D Indoor Spaces Dataset (S3DIS) were used in our experiment. After generating semantic labels on point-clouds using PointNet++ segmentic segmentation algorithm, we use human coders to judge the reconstructability of indoor objects, following a qualitative coding scheme. Human exploration of object shape imperfection was assisted by a visual analytic tool in making their judgement. We found that high point-level accuracy achieved through semantic segmentation of point cloud data does not guarantee high object-level accuracy. The extent of this problem varies widely among different spatial settings and configurations. We discuss the significance of these findings on the choice of 3D reconstruction methods. \n                    <\/jats:p>",
      "source": "CrossRef"
    }
  },
  "codecheck": {
    "codecheckers": [
      {
        "name": "RÃ©my Decoupes",
        "orcid": "0000-0003-0863-9581"
      }
    ],
    "check_time": "2022-07-09 12:00:00",
    "repository": "osf::Z7P8K",
    "report": "https://doi.org/10.17605/osf.io/z7p8k",
    "type": "conference",
    "venue": "AGILEGIS",
    "summary": "The code and data provided by the authors allow to partially reproduce the computational work presented in the Section 4.4 of the paper. The model training (PointNet++) and the input data for Section 4.4 are not reproducible with the provided code but the authors added a note, in the GitHub repository of their project, explaining how the data was generated. Three figures in Section 4.4 are fully reproducible (5, 6, and 7), 2 partially (4 and 8) and 4 are not (3, 9, 10, and 11).\n",
    "manifest": [
      {
        "file": "NA",
        "comment": "The AGILE 2022 Reproducibility Review did not include manifest documentation, see https://github.com/codecheckers/register/issues/38"
      }
    ]
  }
}
